# train.py: Script for training a model using the dataset generated by the previous script.
# This script takes arguments to specify the dataset and other configurations.
#
# example launch command:
#     python scripts/train.py --config demo/sweep/train.yaml

try:
    # This library is our indicator that the required installs
    # need to be done.
    import pyreax

except ModuleNotFoundError:
    # relative import; better to pip install subctrl
    import sys
    sys.path.append("../../pyreax")
    import pyreax

import os, argparse, yaml, json, glob, pickle
import pandas as pd
from tqdm.auto import tqdm
from torch.utils.data import DataLoader
import torch, pyreft
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import get_scheduler
from pathlib import Path

from pyvene import IntervenableModel
from pyreax import (
    EXAMPLE_TAG, 
    ReAXFactory, 
    MaxReLUIntervention, 
    make_data_module, 
    TrainingArgs
)
from pyreax import (
    set_decoder_norm_to_unit_norm, 
    remove_gradient_parallel_to_decoder_directions,
    gather_residual_activations, 
    get_lr
)

import logging
logging.basicConfig(format='%(asctime)s,%(msecs)03d %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s',
    datefmt='%Y-%m-%d:%H:%M:%S',
    level=logging.WARN)
logger = logging.getLogger(__name__)

STATE_FILE = "train_state.pkl"
CONFIG_FILE = "config.json"
DEFAULT_ROTATION_FREQ = 1000


def data_generator(data_dir):
    """
    Generator function to read data files and yield data subsets by group_id.

    Args:
        data_dir (str): Path to the data directory.

    Yields:
        (group_id, df_subset): A tuple containing the group_id and subset DataFrame.
    """
    # Get list of files sorted by index
    file_list = sorted(glob.glob(os.path.join(data_dir, 'train_data_fragment_*.csv')))
    for file_path in file_list:
        df = pd.read_csv(file_path)
        group_ids = df['group_id'].unique()
        group_ids.sort()
        for group_id in group_ids:
            df_subset = df[df['group_id'] == group_id]
            yield (group_id, df_subset)


def load_metadata(metadata_path):
    """
    Load metadata from a JSON lines file.
    """
    metadata = []
    with open(metadata_path, 'r') as f:
        for line in f:
            data = json.loads(line)
            metadata += [data]  # Return the metadata as is
    return metadata


def load_state(dump_dir):
    """
    Load the state from a file if it exists.
    
    Args:
        dump_dir (str): The directory to load the state file from.
    
    Returns:
        dict: The loaded state dictionary, or None if no state file exists.
    """
    state_path = os.path.join(f"{dump_dir}/train", STATE_FILE)
    if os.path.exists(state_path):
        with open(state_path, "rb") as f:
            return pickle.load(f)
    return None
    

def save_reax(args, group_id, intervention, rotation_freq):
    """save artifacts"""
    
    # handle training df first
    dump_dir = args.dump_dir
    dump_dir = Path(dump_dir) / "train"
    dump_dir.mkdir(parents=True, exist_ok=True)
    
    fragment_index = group_id // rotation_freq
    
    # handle weight and bias
    weight_file = Path(dump_dir) / f"weight_fragment_{fragment_index}.pt"
    weight = {}
    if weight_file.exists():
        weight = torch.load(weight_file)
    weight[group_id] = intervention.proj.weight.data.cpu()
    torch.save(weight, weight_file)
    
    bias_file = Path(dump_dir) / f"bias_fragment_{fragment_index}.pt"
    bias = {}
    if bias_file.exists():
        bias = torch.load(bias_file)
    bias[group_id] = intervention.proj.bias.data.cpu()
    torch.save(bias, bias_file)

    state_path = dump_dir / STATE_FILE
    with open(state_path, "wb") as f:
        pickle.dump({"group_id": group_id + 1}, f)

    # save other config
    config = {"model_name": args.model_name,
        "layer": args.layer,
        "component": args.component}
    config_path = dump_dir / CONFIG_FILE
    with open(config_path, 'w') as f:
        json.dump(config, f)


def training_loop(args, train_dataloader, reft_model, reax_intervention):
    torch.cuda.empty_cache()
    
    # Optimizer and lr
    optimizer = torch.optim.AdamW(reft_model.parameters(), lr=args.lr)
    num_training_steps = args.n_epochs * len(train_dataloader)
    lr_scheduler = get_scheduler(
        "linear", optimizer=optimizer,
        num_warmup_steps=0, num_training_steps=num_training_steps)

    # Main training loop.
    progress_bar, curr_step = tqdm(range(num_training_steps)), 0
    for epoch in range(args.n_epochs):
        for batch in train_dataloader:
            # prepare input
            inputs = {k: v.to("cuda") for k, v in batch.items()}
            unit_locations={"sources->base": (
                None,
                inputs["intervention_locations"].permute(1, 0, 2).tolist()
            )}
            subspaces = [{
                "input_subspaces": inputs["input_subspaces"],
                "output_subspaces": inputs["output_subspaces"]}]
    
            # forward
            _, cf_outputs = reft_model(
                base={
                    "input_ids": inputs["input_ids"],
                    "attention_mask": inputs["attention_mask"]
                }, unit_locations=unit_locations, labels=inputs["labels"],
                subspaces=subspaces, use_cache=False)
    
            # loss
            loss = cf_outputs.loss
            latent = reft_model.full_intervention_outputs[0].latent * inputs["intervention_masks"]
            topk_latent, _ = torch.topk(latent, args.k_latent_null_loss, dim=-1)
            null_loss = (topk_latent.mean(dim=-1)*(inputs["groups"]==EXAMPLE_TAG.CONTROL.value))
            null_loss = null_loss.sum()
    
            l1_loss = (latent.mean(dim=-1)*(inputs["groups"]!=EXAMPLE_TAG.CONTROL.value))
            l1_loss = l1_loss.sum()
            
            coeff = curr_step/num_training_steps
            loss += coeff*args.coeff_l1_loss_null*null_loss + coeff*args.coeff_l1_loss*l1_loss
            
            # grads
            loss.backward()
            set_decoder_norm_to_unit_norm(reax_intervention)
            remove_gradient_parallel_to_decoder_directions(reax_intervention)
            curr_step += 1
            curr_lr = get_lr(optimizer)
            # optim
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            progress_bar.update(1)
            progress_bar.set_description("lr %.6f || loss %.6f || null l1 loss %.6f" % (curr_lr, loss, null_loss))


def main():
    args = TrainingArgs()
    logger.warning("Training model with the following configuration:")
    logger.warning(args)

    # Load dataset and metadata.
    metadata_path = os.path.join(args.data_dir, 'metadata.jsonl')
    metadata = load_metadata(metadata_path)
    df_generator = data_generator(args.data_dir)

    # Load lm.
    model = AutoModelForCausalLM.from_pretrained(args.model_name, device_map="cpu")
    model.config.use_cache = False
    model = model.cuda()    
    model = model.eval()
    tokenizer =  AutoTokenizer.from_pretrained(args.model_name)
    tokenizer.padding_side = "right"

    state = load_state(args.dump_dir)
    start_group_id = state.get("group_id", 0) if state else 0
    logger.warning(f"Starting group index: {start_group_id}")
    
    # Iterate over the data and metadata generators
    for (group_id, group_df) in df_generator:
        if group_id < start_group_id:
            continue
        logger.warning(f"Number of records in group_id {group_id}: {len(group_df)}\n")

        # Dataloader.
        data_module = make_data_module(tokenizer, model, group_df)
        train_dataloader = DataLoader(
            data_module["train_dataset"], shuffle=True, batch_size=args.batch_size, 
            collate_fn=data_module["data_collator"])

        # ReFT.
        reax_intervention = MaxReLUIntervention(
            embed_dim=model.config.hidden_size, low_rank_dimension=2,
        )
        reax_intervention = reax_intervention.train()
        reft_config = pyreft.ReftConfig(representations=[{
            "layer": l,
            "component": f"model.layers[{l}].output",
            "low_rank_dimension": 1,
            "intervention": reax_intervention} for l in [args.layer]])
        reft_model = pyreft.get_reft_model(model, reft_config)
        reft_model.set_device("cuda")
        reft_model.print_trainable_parameters()

        # Train.
        training_loop(args, train_dataloader, reft_model, reax_intervention)
        logger.warning("Training finished.")

        # Save.
        save_reax(args, group_id, reax_intervention, DEFAULT_ROTATION_FREQ)

    logger.warning(f"Finished training.")


if __name__ == "__main__":
    main()

