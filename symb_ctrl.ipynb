{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5163c951-1173-44e3-9b7b-8bb760628c6a",
   "metadata": {},
   "source": [
    "## Basic *symbolic*-level control with ReAX.\n",
    "\n",
    "Create an intervention schema such that:\n",
    "\n",
    "**\"whenever the model reads in words related to Stanford, it will say something about human rights issues\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd8dc40-dfc2-4d76-aa0c-e411ae783ee9",
   "metadata": {},
   "source": [
    "#### Set-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4693b498-6e94-4a27-a3c2-c98f6631bf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # This library is our indicator that the required installs\n",
    "    # need to be done.\n",
    "    import pyreax\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    # relative import; better to pip install subctrl\n",
    "    import sys\n",
    "    sys.path.append(\"..\")\n",
    "    import pyreax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5cdecf6-962c-4511-8e01-ca5fca892959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wuzhengx/ipykernel_3362183/3047362727.py:17: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch, pyreft\n",
    "from pyvene import (\n",
    "    IntervenableModel,\n",
    "    ConstantSourceIntervention,\n",
    "    SourcelessIntervention,\n",
    "    TrainableIntervention,\n",
    "    DistributedRepresentationIntervention,\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from circuitsvis.tokens import colored_tokens\n",
    "from IPython.core.display import display, HTML\n",
    "from pyreax import (\n",
    "    EXAMPLE_TAG, \n",
    "    ReAXFactory, \n",
    "    MaxReLUIntervention, \n",
    "    SubspaceAdditionIntervention, \n",
    "    make_data_module, \n",
    "    save_reax\n",
    ")\n",
    "from pyreax import (\n",
    "    set_decoder_norm_to_unit_norm, \n",
    "    remove_gradient_parallel_to_decoder_directions,\n",
    "    gather_residual_activations\n",
    ")\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11aead0-4475-48ba-8d7c-666ff3c87d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb9a7f4945d48759e1722592d0973e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e8466a34dc47b7a6bffb15525e0534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load lm.\n",
    "model_name = \"google/gemma-2-2b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\")\n",
    "model.config.use_cache = False\n",
    "model = model.cuda()\n",
    "\n",
    "tokenizer =  AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661fc614-5539-4568-b2d4-97abb1427219",
   "metadata": {},
   "source": [
    "#### SubCTRL dataset creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdd29567-2d69-4b0c-8f55-612473ebb8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prepare contrast concepts.\n",
      "Skipping contrast concept creation for references related to Stanford and Silicon Valley.\n",
      "Skipping contrast concept creation for references to human rights issues and related organizations.\n",
      "Creating dataframe.\n",
      "Fectching data for 0/2 concept: references related to Stanford and Silicon Valley\n",
      "Fectching data for 1/2 concept: references to human rights issues and related organizations\n",
      "Finished creating current dataframe in 123.985 sec with $0.856.\n"
     ]
    }
   ],
   "source": [
    "reax_factory = ReAXFactory(\n",
    "    model, tokenizer,\n",
    "    concepts=[\n",
    "        # unknown\n",
    "        \"references related to Stanford and Silicon Valley\",   # subspace 1\n",
    "        # https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/11553\n",
    "        \"references to human rights issues and related organizations\", # subspace 2\n",
    "    ], \n",
    "    dump_dir=\"./tmp\",\n",
    "    skip_contrast_concept=True,\n",
    ")\n",
    "\n",
    "# dataset\n",
    "reax_df = reax_factory.create_df(n=66) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b89a8a85-7d33-44e8-b293-06f87ec2616b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_concept</th>\n",
       "      <th>output_concept</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>group</th>\n",
       "      <th>input_subspace</th>\n",
       "      <th>output_subspace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>Drifting through the bustling market, where co...</td>\n",
       "      <td>.\\n\\nThe market is a living tapestry of tradit...</td>\n",
       "      <td>EXAMPLE_TAG.CONTROL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>references related to Stanford and Silicon Valley</td>\n",
       "      <td>references to human rights issues and related ...</td>\n",
       "      <td>Innovative minds gather at Stanford to brainst...</td>\n",
       "      <td>while addressing pressing global challenges l...</td>\n",
       "      <td>EXAMPLE_TAG.EXPERIMENT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>references to human rights issues and related ...</td>\n",
       "      <td>references related to Stanford and Silicon Valley</td>\n",
       "      <td>On sunny weekends, diverse stalls address glob...</td>\n",
       "      <td>highlighting the innovation culture from Stan...</td>\n",
       "      <td>EXAMPLE_TAG.EXPERIMENT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       input_concept  \\\n",
       "0                                               null   \n",
       "1  references related to Stanford and Silicon Valley   \n",
       "2  references to human rights issues and related ...   \n",
       "\n",
       "                                      output_concept  \\\n",
       "0                                               null   \n",
       "1  references to human rights issues and related ...   \n",
       "2  references related to Stanford and Silicon Valley   \n",
       "\n",
       "                                               input  \\\n",
       "0  Drifting through the bustling market, where co...   \n",
       "1  Innovative minds gather at Stanford to brainst...   \n",
       "2  On sunny weekends, diverse stalls address glob...   \n",
       "\n",
       "                                              output                   group  \\\n",
       "0  .\\n\\nThe market is a living tapestry of tradit...     EXAMPLE_TAG.CONTROL   \n",
       "1   while addressing pressing global challenges l...  EXAMPLE_TAG.EXPERIMENT   \n",
       "2   highlighting the innovation culture from Stan...  EXAMPLE_TAG.EXPERIMENT   \n",
       "\n",
       "   input_subspace  output_subspace  \n",
       "0               0                1  \n",
       "1               0                1  \n",
       "2               1                0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reax_df.groupby(['input_concept', 'output_concept']).first().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625ae7b-7cf9-47a0-a59d-f8f96d94646a",
   "metadata": {},
   "source": [
    "#### SubCTRL training.\n",
    "\n",
    "Let's focus on a single layer, layer 20 of the LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23a8e0e6-95d2-4b0a-8ddc-422fef733a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable intervention params: 4,608 || trainable model params: 0\n",
      "model params: 2,614,341,888 || trainable%: 0.0001762585077778473\n"
     ]
    }
   ],
   "source": [
    "layer = 20\n",
    "\n",
    "# make data module.\n",
    "data_module = make_data_module(tokenizer, model, reax_df)\n",
    "train_dataloader = DataLoader(\n",
    "    data_module[\"train_dataset\"], shuffle=True, batch_size=6, \n",
    "    collate_fn=data_module[\"data_collator\"])\n",
    "\n",
    "# get reft model\n",
    "model = model.eval()\n",
    "reax_intervention = MaxReLUIntervention(\n",
    "    embed_dim=model.config.hidden_size, low_rank_dimension=1,\n",
    ")\n",
    "reax_intervention = reax_intervention.train()\n",
    "reft_config = pyreft.ReftConfig(representations=[{\n",
    "    \"layer\": l,\n",
    "    \"component\": f\"model.layers[{l}].output\",\n",
    "    \"low_rank_dimension\": 1,\n",
    "    \"intervention\": reax_intervention} for l in [layer]])\n",
    "reft_model = pyreft.get_reft_model(model, reft_config)\n",
    "reft_model.set_device(\"cuda\")\n",
    "reft_model.print_trainable_parameters()\n",
    "\n",
    "# optimizer and lr\n",
    "num_epochs = 8\n",
    "k_latent = 3\n",
    "optimizer = torch.optim.AdamW(reft_model.parameters(), lr=9e-3)\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer,\n",
    "    num_warmup_steps=0, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e8d42c8-031e-478e-80c9-a73678f7d81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4559e9befa4d4c7395a12e39300af188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main training loop.\n",
    "progress_bar, curr_step = tqdm(range(num_training_steps)), 0\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # prepare input\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "        unit_locations={\"sources->base\": (\n",
    "            None,\n",
    "            inputs[\"intervention_locations\"].permute(1, 0, 2).tolist()\n",
    "        )}\n",
    "        subspaces = [{\n",
    "            \"input_subspaces\": inputs[\"input_subspaces\"],\n",
    "            \"output_subspaces\": inputs[\"output_subspaces\"]}]\n",
    "\n",
    "        # forward\n",
    "        _, cf_outputs = reft_model(\n",
    "            base={\n",
    "                \"input_ids\": inputs[\"input_ids\"],\n",
    "                \"attention_mask\": inputs[\"attention_mask\"]\n",
    "            }, unit_locations=unit_locations, labels=inputs[\"labels\"],\n",
    "            subspaces=subspaces, use_cache=False)\n",
    "\n",
    "        # loss\n",
    "        loss = cf_outputs.loss\n",
    "        latent = reft_model.full_intervention_outputs[0].latent * inputs[\"intervention_masks\"]\n",
    "        topk_latent, _ = torch.topk(latent, k_latent, dim=-1)\n",
    "        null_loss = (topk_latent.mean(dim=-1)*(inputs[\"groups\"]==EXAMPLE_TAG.CONTROL.value))\n",
    "        null_loss = null_loss.sum()\n",
    "        coeff = curr_step/num_training_steps\n",
    "        loss += coeff*0.05*null_loss\n",
    "        \n",
    "        # grads\n",
    "        loss.backward()\n",
    "        set_decoder_norm_to_unit_norm(reax_intervention)\n",
    "        remove_gradient_parallel_to_decoder_directions(reax_intervention)\n",
    "        curr_step += 1\n",
    "        curr_lr = get_lr(optimizer)\n",
    "        # optim\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_description(\"lr %.6f || loss %.6f || null l1 loss %.6f\" % (curr_lr, loss, null_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1485b68-5360-4331-a0c7-b2a2157cf424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dataframe.\n",
      "Fectching data for 0/2 concept: references related to Stanford and Silicon Valley\n",
      "Fectching data for 1/2 concept: references to human rights issues and related organizations\n",
      "Finished creating current dataframe in 16.135 sec with $0.08.\n"
     ]
    }
   ],
   "source": [
    "# create eval dataset\n",
    "eval_reax_df = reax_factory.create_df(n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a469526-5ee4-45e4-962d-bad3a14b737e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> null <null> example:\n",
      "> testing concept: references related to Stanford and Silicon Valley\n",
      "maximal act: 1.537\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-43845882-9eef\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-43845882-9eef\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"Under\", \"\\u2581the\", \"\\u2581orange\", \"\\u2581skies\", \"\\u2581of\", \"\\u2581early\", \"\\u2581autumn\", \",\", \"\\u2581the\", \"\\u2581distant\", \"\\u2581echoes\", \"\\u2581of\", \"\\u2581laughter\", \"\\u2581could\", \"\\u2581be\", \"\\u2581heard\", \",\", \"\\u2581blending\", \"\\u2581with\", \"\\u2581the\", \"\\u2581rustling\", \"\\u2581leaves\", \"\\u2581and\", \"\\u2581the\", \"\\u2581tranquil\", \"\\u2581hum\", \"\\u2581of\", \"\\u2581evening\", \",\", \"\\u2581creating\", \"\\u2581a\", \"\\u2581symphony\", \"\\u2581of\", \"\\u2581peaceful\", \"\\u2581moments\", \"\\u2581in\", \"\\u2581the\", \"\\u2581park\"], \"values\": [0.0, 0.0, 0.0, 0.0, 1.536991000175476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fcb60378eb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> null <null> example:\n",
      "> testing concept: references to human rights issues and related organizations\n",
      "maximal act: 0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-ff6bdd7e-fdc0\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-ff6bdd7e-fdc0\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"Emerging\", \"\\u2581from\", \"\\u2581the\", \"\\u2581dense\", \"\\u2581forest\", \",\", \"\\u2581the\", \"\\u2581mysterious\", \"\\u2581figure\", \"\\u2581moved\", \"\\u2581silently\", \"\\u2581through\", \"\\u2581the\", \"\\u2581under\", \"brush\", \",\", \"\\u2581leaving\", \"\\u2581only\", \"\\u2581a\", \"\\u2581faint\", \"\\u2581trail\", \"\\u2581of\", \"\\u2581footsteps\", \",\", \"\\u2581barely\", \"\\u2581noticeable\", \",\", \"\\u2581as\", \"\\u2581the\", \"\\u2581early\", \"\\u2581morning\", \"\\u2581mist\", \"\\u2581began\", \"\\u2581to\", \"\\u2581dissipate\", \",\", \"\\u2581unveiling\", \"\\u2581the\", \"\\u2581path\", \"\\u2581ahead\"], \"values\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fcb620b1f00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> targeted concept:\n",
      "references related to Stanford and Silicon Valley\n",
      "maximal act: 73.712\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-562f53eb-c275\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-562f53eb-c275\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"In\", \"\\u2581the\", \"\\u2581bustling\", \"\\u2581ecosystem\", \"\\u2581of\", \"\\u2581technology\", \",\", \"\\u2581entrepreneurs\", \"\\u2581often\", \"\\u2581draw\", \"\\u2581inspiration\", \"\\u2581from\", \"\\u2581Stanford\", \"'\", \"s\", \"\\u2581innovative\", \"\\u2581programs\", \"\\u2581and\", \"\\u2581Silicon\", \"\\u2581Valley\", \"'\", \"s\", \"\\u2581dynamic\", \"\\u2581startups\", \",\", \"\\u2581leading\", \"\\u2581to\", \"\\u2581groundbreaking\", \"\\u2581ideas\", \"\\u2581fuelled\", \"\\u2581by\", \"\\u2581collaborative\", \"\\u2581efforts\", \"\\u2581and\", \"\\u2581relentless\", \"\\u2581ambition\"], \"values\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 57.652164459228516, 0.0, 73.7121810913086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 58.05158615112305, 63.91474914550781, 0.0, 0.0, 66.2039566040039, 0.0, 0.0, 0.0, 0.0, 0.0]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fcbc00c8430>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> targeted concept:\n",
      "references related to Stanford and Silicon Valley\n",
      "maximal act: 54.935\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-8f3c6661-0389\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-8f3c6661-0389\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"Emma\", \"\\u2581starts\", \"\\u2581her\", \"\\u2581day\", \"\\u2581with\", \"\\u2581a\", \"\\u2581book\", \",\", \"\\u2581coffee\", \"\\u2581in\", \"\\u2581hand\", \",\", \"\\u2581thinking\", \"\\u2581about\", \"\\u2581stories\", \"\\u2581of\", \"\\u2581Silicon\", \"\\u2581Valley\", \"'\", \"s\", \"\\u2581rise\", \",\", \"\\u2581often\", \"\\u2581influenced\", \"\\u2581by\", \"\\u2581Stanford\", \",\", \"\\u2581before\", \"\\u2581her\", \"\\u2581cat\", \"\\u2581leaps\", \"\\u2581onto\", \"\\u2581a\", \"\\u2581sunny\", \"\\u2581patch\", \"\\u2581on\", \"\\u2581the\", \"\\u2581floor\"], \"values\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 54.934547424316406, 0.0, 48.5508918762207, 0.0, 0.0, 0.0, 0.0, 41.56193161010742, 45.02787399291992, 34.768768310546875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fcb624c83d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> targeted concept:\n",
      "references related to Stanford and Silicon Valley\n",
      "maximal act: 56.074\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-393239e4-2df3\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-393239e4-2df3\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"The\", \"\\u2581artist\", \"\\u2581created\", \"\\u2581a\", \"\\u2581vibrant\", \"\\u2581mural\", \"\\u2581reflecting\", \"\\u2581the\", \"\\u2581mythical\", \"\\u2581story\", \"\\u2581influenced\", \"\\u2581by\", \"\\u2581Silicon\", \"\\u2581Val\", \"leys\", \"\\u2581innovation\", \"\\u2581and\", \"\\u2581Stan\", \"fords\", \"\\u2581academic\", \"\\u2581prowess\", \"\\u2581captivating\", \"\\u2581children\", \"\\u2581with\", \"\\u2581the\", \"\\u2581imaginative\", \"\\u2581display\", \"\\u2581and\", \"\\u2581invoking\", \"\\u2581a\", \"\\u2581sense\", \"\\u2581of\", \"\\u2581wonder\", \"\\u2581in\", \"\\u2581them\"], \"values\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.070098876953125, 0.0, 54.97642517089844, 0.0, 56.07386016845703, 0.0, 31.730764389038086, 0.0, 31.720088958740234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fcb620b2020>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> targeted concept:\n",
      "references to human rights issues and related organizations\n",
      "maximal act: 183.383\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-a7dcd7e9-bd30\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-a7dcd7e9-bd30\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"On\", \"\\u2581their\", \"\\u2581journey\", \"\\u2581through\", \"\\u2581the\", \"\\u2581lush\", \"\\u2581jungle\", \",\", \"\\u2581the\", \"\\u2581adventurers\", \"\\u2581observed\", \"\\u2581locals\", \"\\u2581addressing\", \"\\u2581human\", \"\\u2581rights\", \"\\u2581issues\", \",\", \"\\u2581acknowledging\", \"\\u2581the\", \"\\u2581impact\", \"\\u2581of\", \"\\u2581various\", \"\\u2581international\", \"\\u2581organizations\", \",\", \"\\u2581alongside\", \"\\u2581colorful\", \"\\u2581birds\", \"\\u2581and\", \"\\u2581small\", \"\\u2581mammals\"], \"values\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 183.3826904296875, 57.333587646484375, 45.8560791015625, 0.0, 0.0, 0.0, 0.0, 0.0, 74.76656341552734, 60.05071258544922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fcb62230f40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> targeted concept:\n",
      "references to human rights issues and related organizations\n",
      "maximal act: 176.669\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-8e703035-80b8\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-8e703035-80b8\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"The\", \"\\u2581family\", \"\\u2581decided\", \"\\u2581to\", \"\\u2581go\", \"\\u2581hiking\", \"\\u2581in\", \"\\u2581the\", \"\\u2581mountains\", \"\\u2581where\", \"\\u2581they\", \"\\u2581found\", \"\\u2581murals\", \"\\u2581mentioning\", \"\\u2581human\", \"\\u2581rights\", \"\\u2581organizations\", \",\", \"\\u2581stunning\", \"\\u2581vistas\", \",\", \"\\u2581hidden\", \"\\u2581waterfalls\", \",\", \"\\u2581and\", \"\\u2581ancient\", \"\\u2581ruins\", \"\\u2581that\", \"\\u2581sparked\", \"\\u2581their\", \"\\u2581interest\", \"\\u2581in\", \"\\u2581history\"], \"values\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 65.12858581542969, 176.66864013671875, 75.77447509765625, 42.509822845458984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 41.31730651855469]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fcb60378eb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> targeted concept:\n",
      "references to human rights issues and related organizations\n",
      "maximal act: 177.606\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-723acf9f-0b31\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, ColoredTokens } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-723acf9f-0b31\",\n",
       "      ColoredTokens,\n",
       "      {\"tokens\": [\"The\", \"\\u2581festival\", \"-\", \"goers\", \"\\u2581enjoyed\", \"\\u2581live\", \"\\u2581music\", \"\\u2581while\", \"\\u2581encountering\", \"\\u2581a\", \"\\u2581booth\", \"\\u2581that\", \"\\u2581emphasized\", \"\\u2581human\", \"\\u2581rights\", \"\\u2581issues\", \",\", \"\\u2581offering\", \"\\u2581information\", \"\\u2581about\", \"\\u2581organizations\", \"\\u2581committed\", \"\\u2581to\", \"\\u2581these\", \"\\u2581causes\", \",\", \"\\u2581along\", \"\\u2581with\", \"\\u2581delicious\", \"\\u2581food\", \"\\u2581and\", \"\\u2581unique\", \"\\u2581art\", \"\\u2581installations\"], \"values\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 177.6057891845703, 72.21794128417969, 0.0, 0.0, 0.0, 83.15030670166016, 79.25425720214844, 0.0, 100.680908203125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fcb620b2020>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run inference loop\n",
    "concepts = reax_factory.concepts\n",
    "for _, row in eval_reax_df.iterrows():\n",
    "    prompt = tokenizer.encode(\n",
    "        row[\"input\"], return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\") \n",
    "    if str(row[\"group\"]) == \"EXAMPLE_TAG.CONTROL\":\n",
    "        input_concept = row[\"input_concept\"]\n",
    "        print(f\"> null <{input_concept}> example:\")\n",
    "        test_concept = concepts[row[\"input_subspace\"]]\n",
    "        print(f\"> testing concept: {test_concept}\")\n",
    "    else:\n",
    "        print(f\"> targeted concept:\")\n",
    "        print(concepts[row[\"input_subspace\"]])\n",
    "    target_act = gather_residual_activations(model, layer, prompt)\n",
    "    p, _ = reax_intervention.encode(\n",
    "        target_act[:,1:], \n",
    "        subspaces={\n",
    "            \"input_subspaces\": torch.tensor([row[\"input_subspace\"]]),\n",
    "            \"output_subspaces\": torch.tensor([row[\"output_subspace\"]])}, k=5)\n",
    "    print(\"maximal act:\", round(p.max().tolist(), 3))\n",
    "    html = colored_tokens(tokenizer.tokenize(row[\"input\"]), p.flatten())\n",
    "    display(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe179309-30c9-41a3-8e38-faa94f7c0688",
   "metadata": {},
   "source": [
    "#### Symbolic-like steering with gated interventions on subspaces.\n",
    "\n",
    "We use one subspace to gate the steering with the second subspace. In other words:\n",
    "- We have one subspace acted as the steering wheel.\n",
    "- We have another who tells us how much to steer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d244f4c7-5e1a-44b2-8779-ceded3cf7ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51f441c10374680b9c9634109076baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db9e2602cb9425f86b0e058ab5184c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the chat-lm\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\", # google/gemma-2b-it\n",
    "    device_map='cpu',\n",
    ")\n",
    "chat_model.config.use_cache = False\n",
    "chat_model = chat_model.cuda()\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "_ = chat_model.eval()\n",
    "\n",
    "class SubspaceGatedAdditionIntervention(\n",
    "    SourcelessIntervention,\n",
    "    TrainableIntervention, \n",
    "    DistributedRepresentationIntervention\n",
    "):\n",
    "    def __init__(self, **kwargs):\n",
    "        # Note that we initialise these to zeros because we're loading in pre-trained weights.\n",
    "        # If you want to train your own SAEs then we recommend using blah\n",
    "        super().__init__(**kwargs, keep_last_dim=True)\n",
    "        self.proj = torch.nn.Linear(\n",
    "                self.embed_dim, kwargs[\"low_rank_dimension\"]*2, bias=True)\n",
    "\n",
    "    def forward(self, base, source=None, subspaces=None):\n",
    "        bs, seql, _ = base.shape # b, s, d\n",
    "        base_r = base.reshape(bs*seql, -1).clone()\n",
    "        base_gate = torch.relu(base_r @ self.proj.weight[subspaces[\"gate\"]].unsqueeze(dim=-1)) # b*s, 1\n",
    "        steering_vec = torch.tensor(subspaces[\"mag\"]) * base_gate * self.proj.weight[subspaces[\"steer\"]]\n",
    "        base_s = base_r + steering_vec\n",
    "        output = base_s.reshape(bs, seql, -1)\n",
    "        if seql != 1:\n",
    "            # reset for the prompt tokens only.\n",
    "            output[:,0] = base[:,0]\n",
    "        return output\n",
    "\n",
    "gate_steering_intervention = SubspaceGatedAdditionIntervention(\n",
    "    embed_dim=model.config.hidden_size, low_rank_dimension=1,\n",
    ")\n",
    "gate_steering_intervention.cuda()\n",
    "gate_steering_intervention.proj.weight.data = reax_intervention.proj.weight.data\n",
    "\n",
    "reft_config = pyreft.ReftConfig(representations=[{\n",
    "    \"layer\": l,\n",
    "    \"component\": f\"model.layers[{l}].output\",\n",
    "    \"low_rank_dimension\": 1,\n",
    "    \"intervention\": gate_steering_intervention} for l in [20]])\n",
    "steering_model = pyreft.get_reft_model(chat_model, reft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a2b74-1f90-477a-92fc-7d4f45c93437",
   "metadata": {},
   "source": [
    "#### A simple test case\n",
    "\n",
    "**\"whenever the model reads in words related to Stanford, it will say something about human rights issues\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67f13464-b326-4843-9363-529390511e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mag = references related to Stanford and Silicon Valley\n",
      "+ 2.0 * mag * references to human rights issues and related organizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "List top 10 universities in the U.S. with a short reason.\n",
      "\n",
      "\n",
      "It's impossible to definitively say which are the \"top 10\" as rankings vary depending on factors like:\n",
      "\n",
      "* **Academic reputation:**  How well-regarded is the university for its research and teaching?\n",
      "* **Student selectivity:** How difficult is it to get into the university?\n",
      "* **Resources:** What kind of facilities, funding, and support does the university offer students?\n",
      "* **Faculty quality:** Are there highly qualified professors teaching at the university?\n",
      " \n",
      "However, here are 10 consistently high-ranking US universities that often appear near the top across various lists:\n",
      "\n",
      "**1. Massachusetts Institute of Technology (MIT):** Renowned for its STEM programs and global impact.\n",
      "**2. Stanford University:** Strong focus on innovation, entrepreneurship, and human rights.\n",
      "**3. Harvard University:** A prestigious institution known for its liberal arts education and advocacy.\n",
      "**4. California Institute of Technology (Caltech):** Leading center for science and engineering research.\n",
      "**5. Princeton University:** Known for its strong humanities and social sciences programs.\n",
      "**6. Columbia University:** Excellent in law, journalism, and international relations.\n",
      "**7. Yale University:** Highly regarded for its liberal arts and graduate programs.\n",
      "**8. University of Chicago:** Focuses on critical thinking, economics, and political science.\n",
      "**9. Johns Hopkins University:** Strong in medicine, public health, and international affairs.\n",
      "**10. University of Pennsylvania:** Combines rigorous academics with a focus on promoting social justice.\n",
      "\n",
      "\n",
      "Remember, this list is subjective and based on general consensus. The best university for you depends on your individual interests, goals, and learning style. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"List top 10 universities in the U.S. with a short reason.\"\n",
    "gate_concept_id = 0\n",
    "steer_concept_id = 1\n",
    "mag = 2.0\n",
    "print(f\"mag =\", reax_factory.concepts[gate_concept_id])\n",
    "print(f\"+ {mag} * mag *\", reax_factory.concepts[steer_concept_id])\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True).to(\"cuda\")\n",
    "\n",
    "_, reft_response = steering_model.generate(\n",
    "    prompt, \n",
    "    unit_locations=None, \n",
    "    intervene_on_prompt=True, \n",
    "    subspaces=[{\"gate\": gate_concept_id, \"steer\": steer_concept_id, \"mag\": mag}], max_new_tokens=512, \n",
    "    do_sample=False, early_stopping=True, no_repeat_ngram_size=5, repetition_penalty=1.1\n",
    ")\n",
    "print(tokenizer.decode(reft_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682971e5-ce6e-4037-9d0d-678d446bcb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
