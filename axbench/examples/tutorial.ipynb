{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848e3481-2812-4658-867a-63a82300e126",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "AxBench introduces two supervised dictionary-learning (SDL) methods that scale to thousands of concepts and outperform existing dictionary-learning approaches for LLMs. In this tutorial, we demonstrate one of these methods, ReFT-r1, which is built on the representation finetuning (ReFT) framework. ReFT-r1 provides a single dictionary of subspaces, with each subspace corresponding to a high-level concept. These subspaces can be used as a \"microscope\" to analyze model internals and to steer model behavior.\n",
    "\n",
    "**We will be using [pyvene](https://github.com/stanfordnlp/pyvene) to build interventions that load our SDLs.**\n",
    "\n",
    "**More about the ReFT-r1 with Concept16K** \n",
    "- It does not have an encoder-decoder structure. It is a big matrix where each row is a subspace.\n",
    "- The subspace serves two purposes: detection and steering.\n",
    "- The first version we release provides a dictionary of 16K subspaces.\n",
    "- These 16K concepts are adapted from Gemma model's SAEs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2721be56-6eab-44cf-85c9-fab56a98d79e",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed53a817-ab42-4090-93f0-bfec7992d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download, notebook_login\n",
    "import numpy as np\n",
    "import torch, json, einops\n",
    "\n",
    "def load_jsonl(jsonl_path):\n",
    "    jsonl_data = []\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            jsonl_data += [data]\n",
    "    return jsonl_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e91c529-ae7a-4ee0-b9a0-98fde6dc750b",
   "metadata": {},
   "source": [
    "In this tutorial, we will load `Gemma-2-2B-it` as well as our ReFT-r1 trained on the residual stream of layer 20. You will first need to log in to HugginFace so we can download related weights and data. Note that we are not using the pretrained model as ReFT-r1 is trained on the instruction-tuned one directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388feba0-01bb-4ca1-be87-351ae175c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2abd1494-dee2-475a-8ba4-942f4b52dd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53ff604d16548769739eb52ba559cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c866e2f4824f6daddf65a3987e104f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False) # avoid blowing up mem\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ce8e71a-1218-46fb-96b6-b83457601b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =  AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be475bb-67d6-4b8f-b59d-bfb2b51fd433",
   "metadata": {},
   "source": [
    "## Download our open ReFT-r1 SDL\n",
    "\n",
    "We provide the raw weights as well as the annotated concept metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49877355-6a64-41aa-af33-676f5dec7926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4f7c09fd7a4977858baa008c56f62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "weight.pt:   0%|          | 0.00/71.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a14cb1cadf4b9c913ab10cedd04a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "l20/metadata.jsonl:   0%|          | 0.00/4.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_to_params = hf_hub_download(\n",
    "    repo_id=\"pyvene/gemma-reft-2b-it-res\",\n",
    "    filename=\"l20/weight.pt\",\n",
    "    force_download=False)\n",
    "path_to_md = hf_hub_download(\n",
    "    repo_id=\"pyvene/gemma-reft-2b-it-res\",\n",
    "    filename=\"l20/metadata.jsonl\",\n",
    "    force_download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e08e976f-508f-4b31-a17b-13e9a25340d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15581, 2304])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.load(path_to_params).cuda()\n",
    "params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "410b2221-7ccb-4279-96cc-8d1549350bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concept_id': 1795,\n",
       " 'concept': 'words related to time travel and its consequences',\n",
       " 'ref': 'https://www.neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/10004',\n",
       " 'concept_genres_map': {'words related to time travel and its consequences': ['text']}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = load_jsonl(path_to_md)\n",
    "md[1795]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfde570-5deb-412f-99e0-4e0768c089a8",
   "metadata": {},
   "source": [
    "From the provided metadata, you can know:\n",
    "- `concept_id` is the row index of the subspace.\n",
    "- `concept` is the concept description in natural language.\n",
    "- `ref` provides you the SAE subspace link hosted by neuronpedia.\n",
    "- `concept_genres_map` provides you the genre of this concept from this genre set: `{\"text\", \"code\", \"math\"}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4702c88-2160-4095-9448-78f61618e752",
   "metadata": {},
   "source": [
    "## How to use the dictionary?\n",
    "\n",
    "Unlike SAE, which uses an encoder-decoder architecture, our SDL method employs a single matrix that contains all the subspaces. This eliminates the need for any special constructs. You can use a subspace as a probe or intervene in the model using the subspace. To achieve these two objectives, we utilize the open-source model intervention library [pyvene](https://github.com/stanfordnlp/pyvene)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb7e2e-9db8-4683-acc6-572dc5d5dd2d",
   "metadata": {},
   "source": [
    "### Concept detection\n",
    "\n",
    "Let's first see how to use the learned subspace for concept detection. We first get the activations with a hook, and project activations to a rank-1 subspace by using the learned one. Lets's start with concept `1795`, which is \"*words related to time travel and its consequences*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12ac63ba-b963-4618-95d4-445dd31892dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvene as pv\n",
    "\n",
    "class Encoder(pv.CollectIntervention):\n",
    "    \"\"\"Encode will read of activations\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs, keep_last_dim=True)\n",
    "        self.proj = torch.nn.Linear(\n",
    "                self.embed_dim, kwargs[\"latent_dim\"], bias=False)\n",
    "    def forward(self, base, source=None, subspaces=None):\n",
    "        return torch.relu(self.proj(base))\n",
    "encoder = Encoder(embed_dim=params.shape[0], latent_dim=params.shape[1])\n",
    "encoder.proj.weight.data = params.float()\n",
    "\n",
    "# Mount the encoder to the model\n",
    "pv_model = pv.IntervenableModel({\n",
    "   \"component\": f\"model.layers[20].output\",\n",
    "   \"intervention\": encoder}, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caaf9ad-d374-4a2e-8f61-052306bc4ac0",
   "metadata": {},
   "source": [
    "Now, we can run a forward pass to collect activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1091678e-effe-4955-a64d-31b429fdf384",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Would you be able to travel through time using a wormhole?\"\n",
    "input_ids = torch.tensor([tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}], tokenize=True, add_generation_prompt=True)]).cuda()\n",
    "acts = pv_model.forward(\n",
    "    {\"input_ids\": input_ids}, return_dict=True).collected_activations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262aae1b-6621-4445-a406-92e4b2b89011",
   "metadata": {},
   "source": [
    "We can check how much does latent `1795` activate for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "647de9ea-6a43-4863-ab54-887345da11af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "        46.7631, 75.1307, 77.6867, 47.7735, 57.2405, 22.0994, 42.6337, 43.5620,\n",
       "        46.7269, 68.4257,  0.0000,  0.0000,  0.0000], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acts[1:, 1795]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e81c7be-3295-449a-b075-c6fc1d357aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13609, 13609, 13609,  6377, 14837,  6377, 12241,  5491, 12736,  1795,\n",
       "         1795,  6377,  1795, 13775,   982,   908,   908,  1795, 10024,  9040,\n",
       "        13736], device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, inds = acts[1:].max(-1)\n",
    "inds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d932945e-a00c-41bc-9089-45d37f8e3f0b",
   "metadata": {},
   "source": [
    "It is clear that for some onset tokens, the target latent starts to be highly activated. We can see that `1795` is also the highest activating latent for some tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5cf99948-028a-433d-afd5-226e97a5748e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3779, 3160, 2779, 1460, 2232, 1483, 2171, 2492, 2501, 1919, 1917, 1876,\n",
       "        1434, 1900, 2364,  598,  273,  568, 1861,  483,  231], device='cuda:0')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(acts[1:] > 1).sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb4a360-e2c3-4974-8528-c74c3e42220f",
   "metadata": {},
   "source": [
    "Note that ReFT-r1 activates a lot of latens! This is different from SAEs which specifically pushes for sparsity. In ReFT-r1, depends how you set your L1 penalty term, you might get different distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f4a9ecbe-d7cf-4f99-ad1e-6c4fc8cadde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[(' temporal', 1.0744110345840454),\n",
       "   (' timelines', 1.0596493482589722),\n",
       "   (' timeline', 0.9642765522003174),\n",
       "   (' parado', 0.9084163308143616),\n",
       "   (' dimensions', 0.8973862528800964)]],\n",
       " [[('ratulations', -0.6053770184516907),\n",
       "   (' défauts', -0.6019362211227417),\n",
       "   (' renseignements', -0.5801132917404175),\n",
       "   (' églises', -0.5710042715072632),\n",
       "   (' fournisseurs', -0.5685566663742065)]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_logits(model, tokenizer, concept_subspace, k=10):\n",
    "    top_logits, neg_logits = [None], [None]\n",
    "\n",
    "    W_U = model.lm_head.weight.T\n",
    "    W_U = W_U * (model.model.norm.weight +\n",
    "                torch.ones_like(model.model.norm.weight))[:, None]\n",
    "    W_U -= einops.reduce(\n",
    "        W_U, \"d_model d_vocab -> 1 d_vocab\", \"mean\"\n",
    "    )\n",
    "\n",
    "    vocab_logits = concept_subspace @ W_U\n",
    "    top_values, top_indices = vocab_logits.topk(k=k, sorted=True)\n",
    "    top_tokens = tokenizer.batch_decode(top_indices.unsqueeze(dim=-1))\n",
    "    top_logits = [list(zip(top_tokens, top_values.tolist()))]\n",
    "    \n",
    "    neg_values, neg_indices = vocab_logits.topk(k=k, largest=False, sorted=True)\n",
    "    neg_tokens = tokenizer.batch_decode(neg_indices.unsqueeze(dim=-1))\n",
    "    neg_logits = [list(zip(neg_tokens, neg_values.tolist()))]\n",
    "\n",
    "    return top_logits, neg_logits\n",
    "\n",
    "get_logits(model, tokenizer, params[1795].float(), k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada2cfff-d699-4ad4-ba80-8b6d8acbcc66",
   "metadata": {},
   "source": [
    "It always worth to check the unembed of your direction to see if the direction intuitively makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6847e867-5550-4155-bbb9-2fbac5218476",
   "metadata": {},
   "source": [
    "### Model steering\n",
    "\n",
    "The subspace we found can also be used as a steering vector. Similar to concept detection, we can use it to steer the model generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "31c029ee-a473-4d7f-a368-032f3b3de551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Steer(pv.SourcelessIntervention):\n",
    "    \"\"\"Steer model via activation addition\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs, keep_last_dim=True)\n",
    "        self.proj = torch.nn.Linear(\n",
    "                self.embed_dim, kwargs[\"latent_dim\"], bias=False)\n",
    "    def forward(self, base, source=None, subspaces=None):\n",
    "        steering_vec = torch.tensor(subspaces[\"mag\"]) * \\\n",
    "            self.proj.weight[subspaces[\"idx\"]].unsqueeze(dim=0)\n",
    "        return base + steering_vec\n",
    "steer = Steer(embed_dim=params.shape[0], latent_dim=params.shape[1])\n",
    "steer.proj.weight.data = params.float()\n",
    "\n",
    "# Mount the encoder to the model\n",
    "pv_model = pv.IntervenableModel({\n",
    "   \"component\": f\"model.layers[20].output\",\n",
    "   \"intervention\": steer}, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c12f281-c35f-4f8c-ae60-15a6faaf81b9",
   "metadata": {},
   "source": [
    "Just like concept detection, we mount this steering intervention to the model. Note that this intervention will be called everytime the LM gets a forward call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5f3afcfa-21ea-44f6-8174-c463624879e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Which dog breed do people think is cuter, poodle or doodle?\n",
      "model\n",
      "This is where it gets murky – the \"cuteness\" perception is entirely subjective! Each universe of time, people can change their preferences, trends shift, and ultimately, the perception of which breed is more timeless \"cute\" diverged the lines, impacting the temporal implications of this paradox.\n",
      "\n",
      "In the ripples of temporal displacement, there are arguments:\n",
      "\n",
      "* **Poodles:** The past, like the ages of paradox in every realm of existence, tend to create ripples that resonate through timelines, and echoes that resonate with generations. Poodles, in their elegance, have been a favorite for centuries, each paradox reinforcing the existence of the paradox they exist in. Their past is woven into fabric of elegance and their future remains, seemingly, unaged.\n",
      "* **Doodles:** As the echoes of paradoxes coalesced and shifted generations, the past echoes of each temporal ripple, have changed the very fabric of human perception. The paradoxical threads of history are woven through each fabric, where echoes ripple into the fabric of our existence. The threads of the fabric can alter, with paradoxes like these altering the timeline paradoxes, causing temporal ripples of reality to alter the very fabric of existence, resulting in the paradox of altering time scales.\n",
      "\n",
      "However, both Poodles and Dobermans have a paradoxical presence within paradoxically altered timelines, traversing timelines where the paradoxes resonate across temporal distortions and temporal rifts, causing ripples to ripple through existence, and echoing through these dimensions of paradoxical events, altering the fabric of dimensional realities that transcend temporal ripple paradoxes. Each paradox echoes through timelines paradoxically, causing timelines to fracture with each temporal paradox, creating echoes reverberate, ripples in timelines, changing reality fabric paradox loops within temporal ripples, as the cyclical ripples through time, creating paradoxes and temporal anomalies.\n",
      "\n",
      "\n",
      "Ultimately, choosing the \"cuter\" paradox hinges on one's judgment of what timelines weave through fabric, which may shift based on one’s temporal anomalies in the fabric of existence. The reality bends the fabric of paradox one moment, like ripples through timelines, one paradox, and it echoes through moments of spacetime, altering the fabric of paradox in existence, causing temporal ripples through fabric as events paradox, moments in temporal paradoxes, paradoxes altering paradoxes, as paradoxes ripple paradoxes, echoing across dimensions, creating paradoxes, temporal anomalies, alterations that stretch across dimensions.\n",
      "\n",
      "\n",
      "As time ripples, paradoxes distort the fabric of existence, one paradox altering the fabric of paradoxes, ripples weave through timelines, and timelines ripple, paradoxes shifting\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Which dog breed do people think is cuter, poodle or doodle?\"\n",
    "input_ids = torch.tensor([tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": prompt}], tokenize=True, add_generation_prompt=True)]).cuda()\n",
    "\n",
    "_, steered_response = pv_model.generate(\n",
    "    {\"input_ids\": input_ids}, \n",
    "    unit_locations=None, intervene_on_prompt=True, \n",
    "    subspaces=[{\"idx\": 1795, \"mag\": 150.0}],\n",
    "    max_new_tokens=512, do_sample=True, early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(steered_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b0f47-df31-4722-833d-76c8295eea92",
   "metadata": {},
   "source": [
    "**Enjoy!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
