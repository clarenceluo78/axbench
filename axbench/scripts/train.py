# train.py: Script for training a model using the dataset generated by the previous script.
# This script takes arguments to specify the dataset and other configurations.
#
# example launch command:
#     python axbench/scripts/train.py --config axbench/demo/sweep/train.yaml

try:
    # This library is our indicator that the required installs
    # need to be done.
    import pyreax

except ModuleNotFoundError:
    # relative import; better to pip install subctrl
    import sys
    sys.path.append("../../pyreax")
    import pyreax

import os, argparse, yaml, json, glob, pickle
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer
from pathlib import Path
from args.training_args import TrainingArgs

# all supported methods
import axbench


import logging
logging.basicConfig(format='%(asctime)s,%(msecs)03d %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s',
    datefmt='%Y-%m-%d:%H:%M:%S',
    level=logging.WARN)
logger = logging.getLogger(__name__)

STATE_FILE = "train_state.pkl"
CONFIG_FILE = "config.json"
DEFAULT_ROTATION_FREQ = 1000


def data_generator(data_dir):
    """
    Generator function to read data files and yield data subsets by group_id.

    Args:
        data_dir (str): Path to the data directory.

    Yields:
        (group_id, df_subset): A tuple containing the group_id and subset DataFrame.
    """
    # Get list of files sorted by index
    file_list = sorted(glob.glob(os.path.join(data_dir, 'train_data_fragment_*.csv')))
    for file_path in file_list:
        df = pd.read_csv(file_path)
        group_ids = df['group_id'].unique()
        group_ids.sort()
        for group_id in group_ids:
            df_subset = df[df['group_id'] == group_id]
            yield (group_id, df_subset)


def load_metadata(metadata_path):
    """
    Load metadata from a JSON lines file.
    """
    metadata = []
    with open(metadata_path, 'r') as f:
        for line in f:
            data = json.loads(line)
            metadata += [data]  # Return the metadata as is
    return metadata


def load_state(dump_dir):
    """
    Load the state from a file if it exists.
    
    Args:
        dump_dir (str): The directory to load the state file from.
    
    Returns:
        dict: The loaded state dictionary, or None if no state file exists.
    """
    state_path = os.path.join(f"{dump_dir}/train", STATE_FILE)
    if os.path.exists(state_path):
        with open(state_path, "rb") as f:
            return pickle.load(f)
    return None
    

def save(args, group_id, models, rotation_freq):
    """save artifacts"""
    
    # handle training df first
    dump_dir = args.dump_dir
    dump_dir = Path(dump_dir) / "train"
    dump_dir.mkdir(parents=True, exist_ok=True)
    
    fragment_index = group_id // rotation_freq
    for model in models:
        model.save(dump_dir)

    state_path = dump_dir / STATE_FILE
    with open(state_path, "wb") as f:
        pickle.dump({"group_id": group_id + 1}, f)

    # save other config
    config = {"model_name": args.model_name,
        "layer": args.layer,
        "component": args.component}
    config_path = dump_dir / CONFIG_FILE
    with open(config_path, 'w') as f:
        json.dump(config, f)


def binarize_df(original_df, concept, model_name):
    if model_name in {"LinearProbe", "L1LinearProbe"}:
        # assign input and output containing concept with 1, otherwise 0
        input_df = original_df[original_df["input_concept"]==concept]
        output_df = original_df[original_df["output_concept"]==concept]
        positive_df = pd.concat([input_df["input"], output_df["output"]], axis=0).reset_index(drop=True)
        positive_df = pd.DataFrame(positive_df, columns=['input'])

        input_df = original_df[original_df["input_concept"]!=concept]
        output_df = original_df[original_df["output_concept"]!=concept]
        negative_df = pd.concat([input_df["input"], output_df["output"]], axis=0).reset_index(drop=True)
        negative_df = pd.DataFrame(negative_df, columns=['input'])

        positive_df["labels"] = 1
        negative_df["labels"] = 0

        return pd.concat([positive_df, negative_df], axis=0)
    else:
        # not implemented
        raise NotImplementedError(f"Binarization not implemented for {model_name}")

def main():
    args = TrainingArgs()

    # Load dataset and metadata
    metadata_path = os.path.join(args.data_dir, 'metadata.jsonl')
    metadata = load_metadata(metadata_path)
    df_generator = data_generator(args.data_dir)

    # Load lm.
    model = AutoModelForCausalLM.from_pretrained(args.model_name, device_map="cpu")
    model.config.use_cache = False
    model = model.cuda()    
    model = model.eval()
    tokenizer =  AutoTokenizer.from_pretrained(args.model_name)
    tokenizer.padding_side = "right"

    state = load_state(args.dump_dir)
    start_group_id = state.get("group_id", 0) if state else 0
    logger.warning(f"Starting group index: {start_group_id}")
    
    # Iterate over the data and metadata generators
    
    # We still operate on group_id here, but afterwards, we don't
    # really care about group_id anymore since all evaluations are
    # done at concept level.
    for (group_id, group_df) in df_generator:
        if group_id < start_group_id:
            continue

        # Train.
        benchmark_models = []
        for model_name in args.models.keys():
            model_class = getattr(axbench, model_name)
            if model_name == "ReAX":
                logger.warning(f"Training {model_class} with paired data: group_id {group_id} ({len(group_df)})\n")
                benchmark_model = model_class(
                    model, tokenizer, layer=args.layer, 
                    training_args=args.models[model_name])
                benchmark_model.train(group_df)
                benchmark_models += [benchmark_model]
            else:
                for concept in metadata[group_id]["concepts"]:
                    logger.warning(
                        f"Training {model_class} with non-paired data: group_id/concept {group_id}/{concept} ({len(group_df)})\n")
                    benchmark_model = model_class(
                        model, tokenizer, layer=args.layer, 
                        training_args=args.models[model_name])
                    benchmark_model.train(binarize_df(group_df, concept, model_name))
                    benchmark_models += [benchmark_model]

        # Save
        save(args, group_id, benchmark_models, DEFAULT_ROTATION_FREQ)


if __name__ == "__main__":
    main()

