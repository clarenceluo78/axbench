# inference with test dataset
#     python axbench/scripts/inference.py --config axbench/sweep/inference/gemma-2-2b_20-gemmascope-res-16k.yaml --mode latent
models: [
  # "LinearProbe", "IntegratedGradients", "InputXGradients",
  "L1LinearProbe", "ReAX", "GemmaScopeSAE", 
  "Random", "MeanEmbedding", "MeanActivation", "MeanPositiveActivation"
] 

# for steering,"GemmaScopeSAE" is a must since we benchmark against it for win-rate
model_name: "google/gemma-2-2b"

input_length: 32
output_length: 16

num_of_examples: 100
rotation_freq: 1000
data_dir: "axbench/results/gemma-2-2b_20-gemmascope-res-16k/generate"
train_dir: "axbench/results/gemma-2-2b_20-gemmascope-res-16k/train"
dump_dir: "axbench/results/gemma-2-2b_20-gemmascope-res-16k/"

# steering related params
n_steering_factors: 10
steering_datasets: ["OUATPrefix"]
# "OUATPrefix", "AlpacaEval", "MMLU", "BBQ"

# evaluation related params
eva_batch_size: 64

# master data dir is shared across all jobs.
master_data_dir: "axbench/data"