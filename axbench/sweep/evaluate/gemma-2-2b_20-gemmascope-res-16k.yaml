# evaluate inference results
#    python axbench/scripts/evaluate.py --config axbench/sweep/evaluate/gemma-2-2b_20-gemmascope-res-16k.yaml --mode latent
models: [
  # "LinearProbe", "IntegratedGradients", "InputXGradients",
  "L1LinearProbe", "ReAX", "GemmaScopeSAE", "MeanPositiveActivation", "PromptSteering",
  # "Random", "MeanEmbedding", "MeanActivation", 
] 

latent_evaluators: [
  "AUCROCEvaluator",
  "HardNegativeEvaluator",
]
steering_evaluators: [
  "PerplexityEvaluator", 
  "LMJudgeConceptEvaluator",
  "LMJudgeFollowingEvaluator",
  # "LMJudgeContinuationEvaluator"
]
data_dir: "axbench/results/gemma-2-2b_20-gemmascope-res-16k/inference"
dump_dir: "axbench/results/gemma-2-2b_20-gemmascope-res-16k/"
rotation_freq: 1000

# Number of processes to run in parallel for steering evaluation.
num_of_workers: 32
lm_model: "gpt-4o-mini"
run_winrate: false
# winrate_baseline: "PromptSteering"