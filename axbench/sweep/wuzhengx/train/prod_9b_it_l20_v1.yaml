train:
  model_name: "google/gemma-2-9b-it"
  layer: 20
  component: "res"
  seed: 42
  use_bf16: true
  models:
    # DiffMean:
    #   batch_size: 6
    #   n_epochs: 1
    #   binarize_dataset: true
    #   low_rank_dimension: 1
    # PCA:
    #   batch_size: 6
    #   n_epochs: 1
    #   binarize_dataset: true
    #   low_rank_dimension: 1
    # LAT:
    #   batch_size: 6
    #   n_epochs: 1
    #   binarize_dataset: true
    #   low_rank_dimension: 1
    # SparseLinearProbe:
    #   batch_size: 6
    #   gradient_accumulation_steps: 1
    #   n_epochs: 12
    #   lr: 0.008
    #   topk: 8
    #   coeff_latent_l1_loss: 0.005
    #   coeff_l1_loss: 0.00
    #   coeff_l2_loss: 0.02
    #   binarize_dataset: true
    #   low_rank_dimension: 1
    # LsReFT:
    #   batch_size: 6
    #   gradient_accumulation_steps: 1
    #   n_epochs: 12
    #   lr: 0.008
    #   weight_decay: 0.00
    #   topk: 8
    #   coeff_latent_l1_loss: 0.005
    #   low_rank_dimension: 1
    #   intervention_positions: "all"
    #   intervention_type: "addition" # clamping
    #   binarize_dataset: false
    #   exclude_bos: true
    # SteeringVector:
    #   batch_size: 6
    #   gradient_accumulation_steps: 1
    #   n_epochs: 12
    #   lr: 0.008
    #   weight_decay: 0.00
    #   low_rank_dimension: 1
    #   intervention_positions: "all"
    #   intervention_type: "addition" # clamping
    #   binarize_dataset: false
    #   exclude_bos: true
    IntegratedGradients:
      batch_size: 12
      gradient_accumulation_steps: 6
      weight_decay: 0.00
      n_epochs: 20
      lr: 0.0008
      binarize_dataset: true
      low_rank_dimension: 1