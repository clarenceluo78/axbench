generate:
  lm_model: "gpt-4o-mini"
  output_length: 128
  num_of_examples: 144
  max_concepts: 500
  master_data_dir: "axbench/data"
  dataset_category: "instruction"
  lm_use_cache: false
  seed: 42
  keep_orig_axbench_format: true
  ### use cmd to define ###
  # concept_path: "axbench/data/gemma-2-9b_20-gemmascope-res-16k.json"
train:
  component: "res"
  seed: 42
  use_bf16: true
  use_dpo_loss: true
  use_wandb: false
  output_length: 512 # axbench is 512
  model_name: "google/gemma-3-12b-it"
  steer_dataset_type: "concept"
  models:
    PreferenceVector:
      batch_size: 6 # the actual batch size also needs to multiply with |preference_pairs|
      gradient_accumulation_steps: 1
      n_epochs: 18
      lr: 0.08
      weight_decay: 0.00
      low_rank_dimension: 1
      intervention_positions: "all"
      intervention_type: "addition" # clamping
      binarize_dataset: false
      train_on_negative: true
      exclude_bos: true
      loss_type: "scaled_simpo"
      beta: 1.0
      gemma: 0.0
      simpo_scaler: 1.0
      label_smoothing: 0.0
      dropout: 0.1
      intervention_positions_dropout: 0.0
      steering_factors: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0] # use_cmd_to_define
      preference_pairs: ["orig_add", "orig_sub"] # use_cmd_to_define
      steering_prompt_type: "blend_in"
      substraction_type: "null_it_out" # normal or null_it_out
inference:
  use_bf16: true
  # if you are verifying latent, you dont have to include steering models.
  models: ["PreferenceVector"]
  # latent related params
  output_length: 128
  latent_num_of_examples: 36
  latent_batch_size: 16
  # steering related params
  steering_intervention_type: "addition" # clamping
  steering_datasets: ["AlpacaEval"]
  steering_batch_size: 10
  steering_output_length: 128
  steering_num_of_examples: 10 # number of examples per concept and per factor
  # steering_factors: [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.5, 3.0, 4.0, 5.0] # number of steering factors per example
  # steering_factors: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0] # number of steering factors per example
  steering_factors: [20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 250.0, 300.0, 350.0, 400.0] # number of steering factors per example
  # master data dir is shared across all jobs.
  master_data_dir: "axbench/data"
  seed: 42
  lm_model: "gpt-4o-mini"
  # generation related params
  temperature: 1.0
  ### use cmd to define ###
  steering_model_name: "google/gemma-3-12b-it"
  model_name: "google/gemma-3-12b-it"
  # steering_layer: 20
  # latent_layer: 20
evaluate:
  models: ["PreferenceVector"]
  latent_evaluators: [
    "AUCROCEvaluator",
    "HardNegativeEvaluator",
  ]
  steering_evaluators: [
    # "PerplexityEvaluator", 
    "LMJudgeEvaluator",
  ]
  winrate_split_ratio: 0.5 # this is for steering only, we use a separate partition for factor selection.
  # Number of processes to run in parallel for steering evaluation.
  num_of_workers: 32
  lm_model: "gpt-4o-mini"
  run_winrate: false
  winrate_baseline: "PromptSteering"
  # master data dir is shared across all jobs.
  master_data_dir: "axbench/data"
  steer_dataset_type: "concept"
