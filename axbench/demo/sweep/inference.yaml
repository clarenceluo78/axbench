models: [
  # "LinearProbe", 
  # "IntegratedGradients", 
  # "InputXGradients",
  # "L1LinearProbe", 
  "ReAX", "PromptSteering",
  # "GemmaScopeSAE", "MeanPositiveActivation", 
  # "Random", "MeanEmbedding", "MeanActivation"
] 

model_name: "google/gemma-2-2b"
dump_dir: "axbench/demo"

# latent related params
input_length: 32
latent_num_of_examples: 50
latent_batch_size: 128

# steering related params
steering_model_name: "google/gemma-2-2b-it"
steering_datasets: ["AlpacaEval"]
# "OUATPrefix", "AlpacaEval", "AlpacaEval_Suppress", "AlpacaEval_Synergy", "MMLU", "BBQ"
steering_batch_size: 32
steering_output_length: 128
steering_num_of_examples: 3 # number of examples per concept
steering_factors: [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0] # number of steering factors per example

# master data dir is shared across all jobs.
master_data_dir: "axbench/data"
seed: 42
lm_model: "gpt-4o-mini"