{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9910b2c0-7fef-4544-ac75-7faa32140d59",
   "metadata": {},
   "source": [
    "## Evaluating ReAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf48181-f45b-4d37-9173-cb660e4925e5",
   "metadata": {},
   "source": [
    "#### Set-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c79141-9bff-4d73-a9ff-fe411663fe8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # This library is our indicator that the required installs\n",
    "    # need to be done.\n",
    "    import pyreax\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    # relative import; better to pip install subctrl\n",
    "    import sys\n",
    "    sys.path.append(\"../../pyreax\")\n",
    "    import pyreax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02cfd8e8-e413-437a-90d5-7ce68560b7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/wuzhengx/ipykernel_419885/3683421786.py:19: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch, pyreft\n",
    "from pathlib import Path\n",
    "from pyvene import (\n",
    "    IntervenableModel,\n",
    "    ConstantSourceIntervention,\n",
    "    SourcelessIntervention,\n",
    "    TrainableIntervention,\n",
    "    DistributedRepresentationIntervention,\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from circuitsvis.tokens import colored_tokens\n",
    "from IPython.core.display import display, HTML\n",
    "from pyreax import (\n",
    "    EXAMPLE_TAG, \n",
    "    ReAXFactory, \n",
    "    MaxReLUIntervention, \n",
    "    SubspaceAdditionIntervention, \n",
    "    JumpReLUSAECollectIntervention,\n",
    "    make_data_module, \n",
    "    save_reax,\n",
    "    load_reax,\n",
    "    load_sae,\n",
    "    generate_html_with_highlight_text\n",
    ")\n",
    "from pyreax import (\n",
    "    set_decoder_norm_to_unit_norm, \n",
    "    remove_gradient_parallel_to_decoder_directions,\n",
    "    gather_residual_activations,\n",
    "    get_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb46e64e-b7ae-44dd-9c6a-10cde9ee47b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efede41293ea418a8263393205d89793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681e52fef1d94bb0ad89b34946c270d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# params\n",
    "dump_dir = \"./tmp/gemma-2-2b/20-reax-res-gpt-4o/\"\n",
    "val_n = 10\n",
    "n_decimal = 3\n",
    "reax_topk = 10\n",
    "input_length = 32\n",
    "\n",
    "# Load saved meta.\n",
    "config, training_df, concept_metadata, weight, bias = load_reax(dump_dir)\n",
    "\n",
    "# Load lm.\n",
    "model_name = config.model_name\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\")\n",
    "model.config.use_cache = False\n",
    "model = model.cuda()\n",
    "\n",
    "tokenizer =  AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "sae_weights = load_sae(concept_metadata)\n",
    "\n",
    "LAYER = config.layer\n",
    "\n",
    "reax_intervention = MaxReLUIntervention(\n",
    "    embed_dim=model.config.hidden_size, low_rank_dimension=weight.shape[0],\n",
    ")\n",
    "reax_intervention.proj.weight.data = weight.data\n",
    "reax_intervention.proj.bias.data = bias.data\n",
    "_ = reax_intervention.cuda()\n",
    "pv_reax_model = IntervenableModel({\n",
    "   \"component\": f\"model.layers[{LAYER}].output\",\n",
    "   \"intervention\": reax_intervention}, model=model)\n",
    "\n",
    "sae_intervention = JumpReLUSAECollectIntervention(\n",
    "    embed_dim=sae_weights['W_enc'].shape[0],\n",
    "    low_rank_dimension=sae_weights['W_enc'].shape[1]\n",
    ")\n",
    "sae_intervention.load_state_dict(sae_weights, strict=False)\n",
    "_ = sae_intervention.cuda()\n",
    "pv_sae_model = IntervenableModel({\n",
    "   \"component\": f\"model.layers[{LAYER}].output\",\n",
    "   \"intervention\": sae_intervention}, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259325c-faf8-433e-882e-ea5e1a3cf12e",
   "metadata": {},
   "source": [
    "#### Latent activation eval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f93bf93-23af-455f-8844-98eae494db25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09:02:33:02,692 WARNING  [reax.py:175] Less than 2 concepts are provided. Only eval mode is allowed.\n",
      "2024-10-09:02:33:02,711 WARNING  [reax.py:219] Prepare contrast concepts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with concept: terms related to artificiality and deception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09:02:33:26,780 WARNING  [reax.py:230] Fectching 0 contrast concepts for concept: terms related to artificiality and deception\n",
      "2024-10-09:02:33:26,781 WARNING  [reax.py:235] Finished preparing contrast concepts in 24.069 sec. (current cost: $0.035)\n",
      "2024-10-09:02:33:26,782 WARNING  [reax.py:187] Detecting concept genres.\n",
      "2024-10-09:02:33:27,256 WARNING  [reax.py:194] Finished mapping concept genres in 0.474 sec. (current cost: $0.038)\n",
      "2024-10-09:02:33:27,297 WARNING  [reax.py:240] Creating dataframe.\n",
      "2024-10-09:02:33:40,944 WARNING  [reax.py:240] Creating dataframe.\n",
      "2024-10-09:02:34:10,578 WARNING  [reax.py:240] Creating dataframe.\n",
      "2024-10-09:02:34:10,581 WARNING  [reax.py:175] Less than 2 concepts are provided. Only eval mode is allowed.\n",
      "2024-10-09:02:34:10,604 WARNING  [reax.py:219] Prepare contrast concepts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with concept: terms related to employment and employees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09:02:34:59,234 WARNING  [reax.py:230] Fectching 2 contrast concepts for concept: terms related to employment and employees\n",
      "2024-10-09:02:34:59,235 WARNING  [reax.py:235] Finished preparing contrast concepts in 48.631 sec. (current cost: $0.049)\n",
      "2024-10-09:02:34:59,236 WARNING  [reax.py:187] Detecting concept genres.\n",
      "2024-10-09:02:34:59,604 WARNING  [reax.py:194] Finished mapping concept genres in 0.368 sec. (current cost: $0.053)\n",
      "2024-10-09:02:34:59,605 WARNING  [reax.py:240] Creating dataframe.\n",
      "2024-10-09:02:35:13,413 WARNING  [reax.py:240] Creating dataframe.\n",
      "2024-10-09:02:35:34,895 WARNING  [reax.py:240] Creating dataframe.\n"
     ]
    }
   ],
   "source": [
    "validation_df_map = {}\n",
    "id_sae_link_map = {}\n",
    "for meta in concept_metadata:\n",
    "    meta_dict = json.loads(meta)\n",
    "    concept = meta_dict[\"concept\"]\n",
    "    contrast_concepts = {}\n",
    "    contrast_concepts[concept] = meta_dict[\"contrast_concepts\"]\n",
    "    concept_genres = {}\n",
    "    concept_genres[concept] = meta_dict[\"concept_genres\"]\n",
    "    print(\"Testing with concept:\", concept)\n",
    "    \n",
    "    reax_id = int(meta_dict[\"_id\"])\n",
    "    sae_id = int(meta_dict[\"sae_concept\"].split(\"/\")[-1])\n",
    "    id_sae_link_map[reax_id] = meta_dict[\"sae_concept\"]\n",
    "    \n",
    "    # test prompt\n",
    "    reax_factory = ReAXFactory(\n",
    "        model, tokenizer,\n",
    "        concepts=[concept], \n",
    "        contrast_concepts=contrast_concepts,\n",
    "        dump_dir=dump_dir\n",
    "    )\n",
    "\n",
    "    positive_df = reax_factory.create_eval_df(n=val_n, category=\"positive\", input_length=input_length)\n",
    "    negative_df = reax_factory.create_eval_df(n=val_n, category=\"negative\", input_length=input_length)\n",
    "    hard_negative_df = reax_factory.create_eval_df(n=val_n, category=\"hard negative\", input_length=input_length)\n",
    "    validation_df = pd.concat([positive_df, negative_df, hard_negative_df], axis=0)\n",
    "    validation_df_map[concept] = validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a98ef75-9d60-4eca-8f19-d4193e9a635d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with concept: terms related to artificiality and deception\n",
      "Testing with concept: terms related to employment and employees\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "all_validation_dfs = []\n",
    "with torch.no_grad():\n",
    "    for meta in concept_metadata:\n",
    "        meta_dict = json.loads(meta)\n",
    "        concept = meta_dict[\"concept\"]\n",
    "        contrast_concepts = {}\n",
    "        contrast_concepts[concept] = meta_dict[\"contrast_concepts\"]\n",
    "        print(\"Testing with concept:\", concept)\n",
    "        \n",
    "        reax_id = int(meta_dict[\"_id\"])\n",
    "        sae_id = int(meta_dict[\"sae_concept\"].split(\"/\")[-1]) \n",
    "        validation_df = validation_df_map[concept]\n",
    "        \n",
    "        all_sae_acts = []\n",
    "        all_reax_acts = []\n",
    "        all_sae_max_act = []\n",
    "        all_reax_max_act = []\n",
    "        for _, row in validation_df.iterrows():\n",
    "            inputs = tokenizer.encode(\n",
    "                row[\"input\"], return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n",
    "            # sae acts\n",
    "            sae_acts = pv_sae_model.forward(\n",
    "                {\"input_ids\": inputs}, return_dict=True\n",
    "            ).collected_activations[0][1:, sae_id].data.cpu().numpy().tolist() # no bos token\n",
    "            sae_acts = [round(x, n_decimal) for x in sae_acts]\n",
    "            max_sae_act = max(sae_acts)\n",
    "            \n",
    "            # reax acts\n",
    "            reax_in = gather_residual_activations(model, LAYER, inputs)\n",
    "            reax_acts, _ = reax_intervention.encode(\n",
    "                reax_in[:,1:], # no bos token\n",
    "                subspaces={\n",
    "                    \"input_subspaces\": torch.tensor([reax_id])}, k=reax_topk)\n",
    "            reax_acts = reax_acts.flatten().data.cpu().numpy().tolist()\n",
    "            reax_acts = [round(x, n_decimal) for x in reax_acts]\n",
    "            max_reax_act = max(reax_acts)\n",
    "            \n",
    "            all_sae_acts += [sae_acts]\n",
    "            all_reax_acts += [reax_acts]\n",
    "            all_sae_max_act += [max_sae_act]\n",
    "            all_reax_max_act += [max_reax_act]\n",
    "            \n",
    "        validation_df['sae_acts'] = all_sae_acts\n",
    "        validation_df['reax_acts'] = all_reax_acts\n",
    "        validation_df['max_sae_act'] = all_sae_max_act\n",
    "        validation_df['max_reax_act'] = all_reax_max_act\n",
    "        validation_df['reax_id'] = reax_id\n",
    "        validation_df['sae_id'] = sae_id\n",
    "        validation_df['sae_link'] = meta_dict[\"sae_concept\"]\n",
    "        all_validation_dfs += [validation_df]\n",
    "    \n",
    "    all_validation_df = pd.concat(all_validation_dfs, axis=0)\n",
    "    all_validation_df.to_csv(Path(dump_dir) / f\"val_latent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f993956-1e61-4040-a54a-13ec73533197",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content_interactive = generate_html_with_highlight_text(\n",
    "    id_sae_link_map,\n",
    "    pd.read_csv(Path(dump_dir) / f\"val_latent.csv\"), \n",
    "    tokenizer\n",
    ")\n",
    "output_file_interactive = Path(dump_dir) / f\"val_latent.html\"\n",
    "with open(output_file_interactive, 'w') as file:\n",
    "    file.write(html_content_interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5e68895f-7803-41a4-92c8-19ad6ad93093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "memo = {}\n",
    "\n",
    "async def main_one() -> None:\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Say this is a test\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    print(\"1\", chat_completion)\n",
    "\n",
    "async def main_two() -> None:\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Say this is a test\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    print(\"2\", chat_completion)\n",
    "\n",
    "async def main_three() -> None:\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Say this is a test\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    print(\"3\", chat_completion)\n",
    "\n",
    "async def main_four() -> None:\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Say this is a test\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    print(\"4\", chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "43a6948f-6b10-4d15-aacc-789bd608ed66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-AGecN0kfDEJnydL556nQCseC1pqAj', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='This is a test.', role='assistant', function_call=None, tool_calls=None, refusal=None))], created=1728532747, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=5, prompt_tokens=12, total_tokens=17, prompt_tokens_details={'cached_tokens': 0}, completion_tokens_details={'reasoning_tokens': 0}))\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "422d22d2-edee-4801-9fcf-0c8e12a0b52a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "async def sleep():\n",
    "    print(f'Time: {time.time() - start:.2f}')\n",
    "    await asyncio.sleep(1)\n",
    "\n",
    "async def sum(name, numbers):\n",
    "    total = 0\n",
    "    for number in numbers:\n",
    "        print(f'Task {name}: Computing {total}+{number}')\n",
    "        await sleep()\n",
    "        total += number\n",
    "    print(f'Task {name}: Sum = {total}\\n')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "tasks = [\n",
    "    loop.create_task(sum(\"A\", [1, 2])),\n",
    "    loop.create_task(sum(\"B\", [1, 2, 3])),\n",
    "]\n",
    "loop.run_until_complete(asyncio.wait(tasks))\n",
    "loop.close()\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time: {end-start:.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "50786c8b-1171-4df9-a9de-5dba907cfc46",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 70\u001b[0m\n\u001b[1;32m     63\u001b[0m loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[1;32m     64\u001b[0m tasks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     65\u001b[0m     loop\u001b[38;5;241m.\u001b[39mcreate_task(one()),\n\u001b[1;32m     66\u001b[0m     loop\u001b[38;5;241m.\u001b[39mcreate_task(two()),\n\u001b[1;32m     67\u001b[0m     loop\u001b[38;5;241m.\u001b[39mcreate_task(three()),\n\u001b[1;32m     68\u001b[0m     loop\u001b[38;5;241m.\u001b[39mcreate_task(four()),\n\u001b[1;32m     69\u001b[0m ]\n\u001b[0;32m---> 70\u001b[0m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m loop\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     73\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/asyncio/base_events.py:625\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m--> 625\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m new_task \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m futures\u001b[38;5;241m.\u001b[39misfuture(future)\n\u001b[1;32m    628\u001b[0m future \u001b[38;5;241m=\u001b[39m tasks\u001b[38;5;241m.\u001b[39mensure_future(future, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/asyncio/base_events.py:584\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[0;32m--> 584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis event loop is already running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    587\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot run the event loop while another loop is running\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ChatCompletion(id='chatcmpl-AGiANfnPp5pWDKOmXYtLIHhK6S3Mc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='This is another test.', role='assistant', function_call=None, tool_calls=None, refusal=None))], created=1728546387, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=5, prompt_tokens=12, total_tokens=17, prompt_tokens_details={'cached_tokens': 0}, completion_tokens_details={'reasoning_tokens': 0}))\n",
      "1 ChatCompletion(id='chatcmpl-AGiAN9kwwhHqNPfPyJ9wXJXtgwy33', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='This is a test.', role='assistant', function_call=None, tool_calls=None, refusal=None))], created=1728546387, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=5, prompt_tokens=12, total_tokens=17, prompt_tokens_details={'cached_tokens': 0}, completion_tokens_details={'reasoning_tokens': 0}))\n",
      "4 ChatCompletion(id='chatcmpl-AGiANPmy3Ht8L4kATzrWdXefJesDi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='This is the final test.', role='assistant', function_call=None, tool_calls=None, refusal=None))], created=1728546387, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=6, prompt_tokens=13, total_tokens=19, prompt_tokens_details={'cached_tokens': 0}, completion_tokens_details={'reasoning_tokens': 0}))\n",
      "3 ChatCompletion(id='chatcmpl-AGiAN7srK3vWLRNk7Cy9lbKcas0DK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='This is yet another test.', role='assistant', function_call=None, tool_calls=None, refusal=None))], created=1728546387, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=6, prompt_tokens=13, total_tokens=19, prompt_tokens_details={'cached_tokens': 0}, completion_tokens_details={'reasoning_tokens': 0}))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "async def main_one() -> dict:\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Say this is a test\"}\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    print(\"1\", chat_completion)\n",
    "    return chat_completion\n",
    "\n",
    "async def main_two() -> dict:\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Say this is another test\"}\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    print(\"2\", chat_completion)\n",
    "    return chat_completion\n",
    "\n",
    "async def main_three() -> dict:\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Say this is yet another test\"}\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    print(\"3\", chat_completion)\n",
    "    return chat_completion\n",
    "\n",
    "async def main_four() -> dict:\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Say this is the final test\"}\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    print(\"4\", chat_completion)\n",
    "    return chat_completion\n",
    "\n",
    "async def one():\n",
    "    await main_one()\n",
    "\n",
    "async def two():\n",
    "    await main_two()\n",
    "\n",
    "async def three():\n",
    "    await main_three()\n",
    "\n",
    "async def four():\n",
    "    await main_four()\n",
    "    \n",
    "start = time.time()\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "tasks = [\n",
    "    loop.create_task(one()),\n",
    "    loop.create_task(two()),\n",
    "    loop.create_task(three()),\n",
    "    loop.create_task(four()),\n",
    "]\n",
    "loop.run_until_complete(asyncio.wait(tasks))\n",
    "loop.close()\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time: {end-start:.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e191335-4e54-45fb-a076-f2c6320c8804",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot close a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/asyncio/unix_events.py:68\u001b[0m, in \u001b[0;36m_UnixSelectorEventLoop.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mis_finalizing():\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_handlers):\n",
      "File \u001b[0;32m/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/asyncio/selector_events.py:84\u001b[0m, in \u001b[0;36mBaseSelectorEventLoop.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_running():\n\u001b[0;32m---> 84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot close a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_closed():\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot close a running event loop"
     ]
    }
   ],
   "source": [
    "loop.close()\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time: {end-start:.2f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27725ec9-a30c-45e8-b825-452bd3659ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
