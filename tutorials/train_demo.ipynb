{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f574b3b-7c21-4027-8f91-974cdb425b62",
   "metadata": {},
   "source": [
    "## Introducing ReAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1086f1-c347-4548-9ce0-1a60db93f2af",
   "metadata": {},
   "source": [
    "#### Set-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ce12f3-b73a-4389-a885-2d6eb22202ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # This library is our indicator that the required installs\n",
    "    # need to be done.\n",
    "    import pyreax\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    # relative import; better to pip install subctrl\n",
    "    import sys\n",
    "    sys.path.append(\"../../pyreax\")\n",
    "    import pyreax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dcd5a4-7af1-45f2-9c75-e283a605a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch, pyreft\n",
    "from pyvene import (\n",
    "    IntervenableModel,\n",
    "    ConstantSourceIntervention,\n",
    "    SourcelessIntervention,\n",
    "    TrainableIntervention,\n",
    "    DistributedRepresentationIntervention,\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from circuitsvis.tokens import colored_tokens\n",
    "from IPython.core.display import display, HTML\n",
    "from pyreax import (\n",
    "    EXAMPLE_TAG, \n",
    "    ReAXFactory, \n",
    "    MaxReLUIntervention, \n",
    "    SubspaceAdditionIntervention, \n",
    "    make_data_module, \n",
    "    save_reax,\n",
    "    Config,\n",
    "    load_config_from_json,\n",
    "    load_concepts,\n",
    ")\n",
    "from pyreax import (\n",
    "    set_decoder_norm_to_unit_norm, \n",
    "    remove_gradient_parallel_to_decoder_directions,\n",
    "    gather_residual_activations, \n",
    "    get_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a48af6-75a9-4a9a-8fff-2c947de6afe3",
   "metadata": {},
   "source": [
    "#### Training.\n",
    "\n",
    "Let's focus on a single layer, layer 20 of the LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8551d1-b09f-42c9-b614-a3d9bb347e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    lm_model = \"gpt-4o\",\n",
    "    concept_path = \"../demo/url_concepts.txt\",\n",
    "    model_name = \"google/gemma-2-2b\",\n",
    "    n_data = 66,\n",
    "    layer = 20,\n",
    "    component = \"res\",\n",
    "    input_length = 32,\n",
    "    output_length = 16,\n",
    "    \n",
    "    batch_size = 6,\n",
    "    n_epochs = 12,\n",
    "    k_latent_null_loss = 1,\n",
    "    lr = 3E-3,\n",
    "    coeff_l1_loss_null = 5E-2,\n",
    "    coeff_l1_loss = 1E-3,\n",
    "    dump_dir = \"./tmp\"\n",
    ")\n",
    "\n",
    "# params\n",
    "lm_model = config.lm_model\n",
    "concept_path = config.concept_path\n",
    "model_name = config.model_name\n",
    "N = config.n_data\n",
    "lr = config.lr\n",
    "layer = config.layer\n",
    "num_epochs = config.n_epochs\n",
    "k_latent = config.k_latent_null_loss\n",
    "coeff_l1_loss_null = config.coeff_l1_loss_null\n",
    "coeff_l1_loss = config.coeff_l1_loss\n",
    "dump_dir = config.dump_dir\n",
    "input_length = config.input_length\n",
    "output_length = config.output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff7d22d-b711-4bad-9fa0-2d12979ab13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for demo purposes, these are import directly\n",
    "concepts, sae_metadata = load_concepts(concept_path)\n",
    "\n",
    "# Load lm.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cpu\")\n",
    "model.config.use_cache = False\n",
    "model = model.cuda()\n",
    "\n",
    "tokenizer =  AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# create dataset\n",
    "reax_factory = ReAXFactory(\n",
    "    model, tokenizer,\n",
    "    lm_model=lm_model,\n",
    "    concepts=concepts, \n",
    "    dump_dir=dump_dir,\n",
    ")\n",
    "reax_df = reax_factory.create_df(\n",
    "    n=N,\n",
    "    input_length=input_length,\n",
    "    output_length=output_length\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4e4fe9-b7a0-48c3-afa9-c674e4251676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data module.\n",
    "data_module = make_data_module(tokenizer, model, reax_df)\n",
    "train_dataloader = DataLoader(\n",
    "    data_module[\"train_dataset\"], shuffle=True, batch_size=config.batch_size, \n",
    "    collate_fn=data_module[\"data_collator\"])\n",
    "\n",
    "# get reft model\n",
    "model = model.eval()\n",
    "reax_intervention = MaxReLUIntervention(\n",
    "    embed_dim=model.config.hidden_size, low_rank_dimension=2,\n",
    ")\n",
    "reax_intervention = reax_intervention.train()\n",
    "reft_config = pyreft.ReftConfig(representations=[{\n",
    "    \"layer\": l,\n",
    "    \"component\": f\"model.layers[{l}].output\",\n",
    "    \"low_rank_dimension\": 1,\n",
    "    \"intervention\": reax_intervention} for l in [layer]])\n",
    "reft_model = pyreft.get_reft_model(model, reft_config)\n",
    "reft_model.set_device(\"cuda\")\n",
    "reft_model.print_trainable_parameters()\n",
    "\n",
    "# optimizer and lr\n",
    "optimizer = torch.optim.AdamW(reft_model.parameters(), lr=config.lr)\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer,\n",
    "    num_warmup_steps=0, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34a054-c785-4111-8181-590a68eef670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop.\n",
    "progress_bar, curr_step = tqdm(range(num_training_steps)), 0\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # prepare input\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "        unit_locations={\"sources->base\": (\n",
    "            None,\n",
    "            inputs[\"intervention_locations\"].permute(1, 0, 2).tolist()\n",
    "        )}\n",
    "        subspaces = [{\n",
    "            \"input_subspaces\": inputs[\"input_subspaces\"],\n",
    "            \"output_subspaces\": inputs[\"output_subspaces\"]}]\n",
    "\n",
    "        # forward\n",
    "        _, cf_outputs = reft_model(\n",
    "            base={\n",
    "                \"input_ids\": inputs[\"input_ids\"],\n",
    "                \"attention_mask\": inputs[\"attention_mask\"]\n",
    "            }, unit_locations=unit_locations, labels=inputs[\"labels\"],\n",
    "            subspaces=subspaces, use_cache=False)\n",
    "\n",
    "        # loss\n",
    "        loss = cf_outputs.loss\n",
    "        latent = reft_model.full_intervention_outputs[0].latent * inputs[\"intervention_masks\"]\n",
    "        topk_latent, _ = torch.topk(latent, k_latent, dim=-1)\n",
    "        null_loss = (topk_latent.mean(dim=-1)*(inputs[\"groups\"]==EXAMPLE_TAG.CONTROL.value))\n",
    "        null_loss = null_loss.sum()\n",
    "\n",
    "        l1_loss = (latent.mean(dim=-1)*(inputs[\"groups\"]!=EXAMPLE_TAG.CONTROL.value))\n",
    "        l1_loss = l1_loss.sum()\n",
    "        \n",
    "        coeff = curr_step/num_training_steps\n",
    "        loss += coeff*coeff_l1_loss_null*null_loss + coeff*coeff_l1_loss*l1_loss\n",
    "        \n",
    "        # grads\n",
    "        loss.backward()\n",
    "        set_decoder_norm_to_unit_norm(reax_intervention)\n",
    "        remove_gradient_parallel_to_decoder_directions(reax_intervention)\n",
    "        curr_step += 1\n",
    "        curr_lr = get_lr(optimizer)\n",
    "        # optim\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_description(\"lr %.6f || loss %.6f || null l1 loss %.6f\" % (curr_lr, loss, null_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d05c2-4f80-4719-84b7-4d6cec7dc37a",
   "metadata": {},
   "source": [
    "#### Eval - latent space disentanglements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb98446c-067d-48d6-9f0d-0bca964fccbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run inference loop\n",
    "concepts = reax_factory.concepts\n",
    "for _, row in reax_df.iterrows():\n",
    "    prompt = tokenizer.encode(\n",
    "        row[\"input\"], return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\") \n",
    "    if str(row[\"group\"]) == \"EXAMPLE_TAG.CONTROL\":\n",
    "        input_concept = row[\"input_concept\"]\n",
    "        print(f\"> null <{input_concept}> example:\")\n",
    "        test_concept = concepts[row[\"input_subspace\"]]\n",
    "        print(f\"> testing concept: {test_concept}\")\n",
    "    else:\n",
    "        print(f\"> targeted concept:\")\n",
    "        print(concepts[row[\"input_subspace\"]])\n",
    "    target_act = gather_residual_activations(model, layer, prompt)\n",
    "    p, _ = reax_intervention.encode(\n",
    "        target_act[:,1:], \n",
    "        subspaces={\n",
    "            \"input_subspaces\": torch.tensor([row[\"input_subspace\"]]),\n",
    "            \"output_subspaces\": torch.tensor([row[\"output_subspace\"]])}, k=10)\n",
    "    print(\"maximal act:\", round(p.max().tolist(), 3))\n",
    "    html = colored_tokens(tokenizer.tokenize(row[\"input\"]), p.flatten())\n",
    "    display(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03326af6-33f5-4343-949b-296543c171ab",
   "metadata": {},
   "source": [
    "#### Eval - logits lens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ff820-8e98-4c00-8ecb-867344502ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_logits = model.lm_head.weight @ reax_intervention.proj.weight.data[0]\n",
    "values, indices = vocab_logits.topk(k=10)\n",
    "tokenizer.batch_decode(indices.unsqueeze(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c4f0f-86ac-4493-b7c4-807c8ab1c706",
   "metadata": {},
   "source": [
    "#### Eval - steering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882017a-3c1a-408a-8a17-d97c5250c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the chat-lm\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\", # google/gemma-2b-it\n",
    "    device_map='cpu',\n",
    ")\n",
    "chat_model.config.use_cache = False\n",
    "chat_model = chat_model.cuda()\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "_ = chat_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02161750-dd25-48ad-8e38-a49e0d24039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_intervention = SubspaceAdditionIntervention(\n",
    "    embed_dim=model.config.hidden_size, low_rank_dimension=2,\n",
    ")\n",
    "steering_intervention.cuda()\n",
    "steering_intervention.proj.weight.data = reax_intervention.proj.weight.data\n",
    "\n",
    "reft_config = pyreft.ReftConfig(representations=[{\n",
    "    \"layer\": l,\n",
    "    \"component\": f\"model.layers[{l}].output\",\n",
    "    \"low_rank_dimension\": 1,\n",
    "    \"intervention\": steering_intervention} for l in [20]])\n",
    "steering_model = pyreft.get_reft_model(chat_model, reft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52887862-bbc3-4345-95d4-cbfb8bfd3e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a English story.\"\n",
    "concept_id = 0\n",
    "mag = 120\n",
    "print(f\"+ {mag} *\", reax_factory.concepts[concept_id])\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True).to(\"cuda\")\n",
    "\n",
    "_, reft_response = steering_model.generate(\n",
    "    prompt, \n",
    "    unit_locations=None, \n",
    "    intervene_on_prompt=True, \n",
    "    subspaces=[{\"idx\": concept_id, \"mag\": mag}], max_new_tokens=128, \n",
    "    do_sample=False, early_stopping=True, no_repeat_ngram_size=5, repetition_penalty=1.1\n",
    ")\n",
    "print(tokenizer.decode(reft_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825fd52-4347-433c-bd47-92930300040d",
   "metadata": {},
   "source": [
    "#### Saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc17f6c-2a08-4a28-91b5-d88a6395a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "save_reax(\"./tmp\", config, reax_df, reax_factory, sae_metadata, reax_intervention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98bb18b-2210-4b9b-a321-195d0a7706d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
