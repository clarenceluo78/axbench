# train.py: Script for training a model using the dataset generated by the previous script.
# This script takes arguments to specify the dataset and other configurations.
#
# example launch command:
#     python train.py --data_dir demo --dump_dir demo --config demo/sweep/train.yaml

try:
    # This library is our indicator that the required installs
    # need to be done.
    import pyreax

except ModuleNotFoundError:
    # relative import; better to pip install subctrl
    import sys
    sys.path.append("../../pyreax")
    import pyreax

import os, argparse, yaml, json, glob
import pandas as pd
from tqdm.auto import tqdm
from torch.utils.data import DataLoader
import torch, pyreft
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import get_scheduler

from pyvene import IntervenableModel
from pyreax import (
    EXAMPLE_TAG, 
    ReAXFactory, 
    MaxReLUIntervention, 
    SubspaceAdditionIntervention, 
    make_data_module, 
    save_reax,
    Config,
    load_config_from_json,
    load_concepts,
    TrainingArgs
)
from pyreax import (
    set_decoder_norm_to_unit_norm, 
    remove_gradient_parallel_to_decoder_directions,
    gather_residual_activations, 
    get_lr
)

import logging
logging.basicConfig(format='%(asctime)s,%(msecs)03d %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s',
    datefmt='%Y-%m-%d:%H:%M:%S',
    level=logging.WARN)
logger = logging.getLogger(__name__)


def data_generator(data_dir):
    """
    Generator function to read data files and yield data subsets by group_id.

    Args:
        data_dir (str): Path to the data directory.

    Yields:
        (group_id, df_subset): A tuple containing the group_id and subset DataFrame.
    """
    # Get list of files sorted by index
    file_list = sorted(glob.glob(os.path.join(data_dir, 'train_data_fragment_*.csv')))
    for file_path in file_list:
        df = pd.read_csv(file_path)
        group_ids = df['group_id'].unique()
        group_ids.sort()
        for group_id in group_ids:
            df_subset = df[df['group_id'] == group_id]
            yield (group_id, df_subset)


def load_metadata(metadata_path):
    """
    Load metadata from a JSON lines file.
    """
    metadata = []
    with open(metadata_path, 'r') as f:
        for line in f:
            data = json.loads(line)
            metadata += [data]  # Return the metadata as is
    return metadata


def training_loop(args, train_dataloader, reft_model, reax_intervention):
    # Optimizer and lr
    optimizer = torch.optim.AdamW(reft_model.parameters(), lr=args.lr)
    num_training_steps = args.n_epochs * len(train_dataloader)
    lr_scheduler = get_scheduler(
        "linear", optimizer=optimizer,
        num_warmup_steps=0, num_training_steps=num_training_steps)

    # Main training loop.
    progress_bar, curr_step = tqdm(range(num_training_steps)), 0
    for epoch in range(args.n_epochs):
        for batch in train_dataloader:
            # prepare input
            inputs = {k: v.to("cuda") for k, v in batch.items()}
            unit_locations={"sources->base": (
                None,
                inputs["intervention_locations"].permute(1, 0, 2).tolist()
            )}
            subspaces = [{
                "input_subspaces": inputs["input_subspaces"],
                "output_subspaces": inputs["output_subspaces"]}]
    
            # forward
            _, cf_outputs = reft_model(
                base={
                    "input_ids": inputs["input_ids"],
                    "attention_mask": inputs["attention_mask"]
                }, unit_locations=unit_locations, labels=inputs["labels"],
                subspaces=subspaces, use_cache=False)
    
            # loss
            loss = cf_outputs.loss
            latent = reft_model.full_intervention_outputs[0].latent * inputs["intervention_masks"]
            topk_latent, _ = torch.topk(latent, args.k_latent_null_loss, dim=-1)
            null_loss = (topk_latent.mean(dim=-1)*(inputs["groups"]==EXAMPLE_TAG.CONTROL.value))
            null_loss = null_loss.sum()
    
            l1_loss = (latent.mean(dim=-1)*(inputs["groups"]!=EXAMPLE_TAG.CONTROL.value))
            l1_loss = l1_loss.sum()
            
            coeff = curr_step/num_training_steps
            loss += coeff*args.coeff_l1_loss_null*null_loss + coeff*args.coeff_l1_loss*l1_loss
            
            # grads
            loss.backward()
            set_decoder_norm_to_unit_norm(reax_intervention)
            remove_gradient_parallel_to_decoder_directions(reax_intervention)
            curr_step += 1
            curr_lr = get_lr(optimizer)
            # optim
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            progress_bar.update(1)
            progress_bar.set_description("lr %.6f || loss %.6f || null l1 loss %.6f" % (curr_lr, loss, null_loss))


def main():
    # Initialize TrainingArgs, which will parse args and load config
    args = TrainingArgs()
    logger.warning("Training model with the following configuration:")
    logger.warning(args)

    # Load dataset and metadata.
    metadata_path = os.path.join(args.data_dir, 'metadata.jsonl')
    metadata = load_metadata(metadata_path)
    df_generator = data_generator(args.data_dir)

    # Load lm.
    model = AutoModelForCausalLM.from_pretrained(args.model_name, device_map="cpu")
    model.config.use_cache = False
    model = model.cuda()    
    model = model.eval()
    tokenizer =  AutoTokenizer.from_pretrained(args.model_name)
    tokenizer.padding_side = "right"

    # Iterate over the data and metadata generators
    for (group_id, group_df) in df_generator:
        logger.warning(f"Number of records in group_id {group_id}: {len(group_df)}\n")

        # Dataloader.
        data_module = make_data_module(tokenizer, model, group_df)
        train_dataloader = DataLoader(
            data_module["train_dataset"], shuffle=True, batch_size=args.batch_size, 
            collate_fn=data_module["data_collator"])

        # ReFT.
        reax_intervention = MaxReLUIntervention(
            embed_dim=model.config.hidden_size, low_rank_dimension=2,
        )
        reax_intervention = reax_intervention.train()
        reft_config = pyreft.ReftConfig(representations=[{
            "layer": l,
            "component": f"model.layers[{l}].output",
            "low_rank_dimension": 1,
            "intervention": reax_intervention} for l in [args.layer]])
        reft_model = pyreft.get_reft_model(model, reft_config)
        reft_model.set_device("cuda")
        reft_model.print_trainable_parameters()

        # Train.
        training_loop(args, train_dataloader, reft_model, reax_intervention)
        logger.warning("Training finished.")

        # Save.
                


if __name__ == "__main__":
    main()

